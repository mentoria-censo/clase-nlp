---
# title: "Capacitación en R y herramientas de productividad"
# author: "Abril 2021"
format:
  # html:
  #   code-fold: true
  revealjs:
    auto-stretch: false
    margin: 0
    slide-number: true
    scrollable: true
    preview-links: auto
    page-layout: custom
    logo: imagenes/logo_portada2.png
    css: ine_quarto_styles.css
    # footer: <https://quarto.org>

engine: knitr
---

# 

```{=html}
<!---
# TODO: this does not work
 .linea-superior[]
.linea-inferior[] 
--->
```
```{=html}
<!---
# TODO: this does not work
 ![](imagenes/logo_portada2.png){.center style="width: 20%;"}   
--->
```
```{=html}
<!---
 <img src="imagenes/logo_portada2.png" style="width: 20%"/>  
--->
```
[<img src="imagenes/logo_portada2.png" width="20%"/>]{.center-justified}

[**Mentoría Censo: Codificación**]{.big-par .center-justified}

[**Proyecto Ciencia de Datos**]{.big-par .center-justified}

[**Introducción al Procesamiento de Lenguaje Natural**]{.big-par .center-justified}

[**octubre 2023**]{.big-par .center-justified}

## Un poco de código

```{r, echo=T}

if (!require("quanteda")) install.packages("quanteda")
if (!require("tidyverse")) install.packages("tidyverse")
if (!require("quanteda.textstats")) install.packages("quanteda.textstats")
if (!require("kableExtra")) install.packages("kableExtra")
if (!require("word2vec")) install.packages("word2vec")
if (!require("reticulate")) install.packages("reticulate")
if (!require("tm")) install.packages("tm")

crear_tabla <- function(tabla) {
  tabla %>% 
    kbl() %>% 
    kable_styling(font_size = 18)
}



```

## ¿Qué es NLP?

::: medium-par
El Procesamiento del Lenguaje Natural (NLP, por sus siglas en inglés) se enfoca en la interacción entre el lenguaje de los computadores y el de los humanos.

Considera tanto algoritmos como modelos que permiten a los computadores comprender, interpretar y generar el lenguaje humano de una manera que sea significativa y útil.

Como podrán haber notado, este campo ha avanzado increíblemente en los últimos años.

(Esta clase se centrará en avances previos a la revolución que significó ChatGPT)
:::

::: r-stack
::: {.fragment .fade-out}
<img src="imagenes/ELIZA_conversation.png" width="50%"/>
:::

::: {.fragment .fade-in-then-out}
<img src="imagenes/word_vector.png" width="50%"/>
:::

::: {.fragment .fade-in-then-out}
<img src="imagenes/embeddings.png" width="60%"/>
:::

::: {.fragment .fade-in-then-out}
<img src="imagenes/chatgpt.jpg" width="30%"/>
:::
:::

## Importancia y aplicaciones de NLP

::: r-stack
::: {.fade-out .fade-in-then-out .fragment}
El NLP actualmente se encuentra en todas partes en internet, resultando fundamental en el panorama tecnológico actual:

::: medium-par
-   **Análisis de Sentimiento**: Comprender el sentimiento público hacia productos o temas (p/e animotuitero INEGI).
-   **Traducción Automática**: Traducir texto de un idioma a otro (p/e Google translate).
-   **Chatbots y Asistentes Virtuales**: Permitir interacciones similares a las humanas con máquinas (p/e ChatGPT, Siri).
-   **Extracción de Información**: Extraer datos estructurados de texto no estructurado (p/e *web scraping*).
-   **Motores de Búsqueda**: Mejorar la relevancia de los resultados de búsqueda.
-   Autocorrectores, modelos de clasificación y un largo etcétera.
:::
:::

::: fragment
<img src="imagenes/ejemplos_nlp.png" width="70%"/>
:::
:::

## Desafíos del Procesamiento del Lenguaje Natural

Se ha avanzado mucho en NLP, pero siguen habien múltiples desafíos, incluyendo:

::: {.medium-par .incremental}
-   **Ambigüedad**: El lenguaje suele ser ambiguo, lo que lleva a dificultades en la interpretación (p/e homónimos).
-   **Calidad de los Datos y Sesgos**: Los modelos de NLP pueden estar sesgados debido a datos de entrenamiento sesgados (p/e ENE vs Censo).
-   **Comprensión del Contexto**: Comprender el contexto y el sarcasmo es complicado.
-   **Multilingüismo**: Manejar múltiples idiomas y dialectos de manera efectiva (acentos y eñes siempre un problema).
-   **Privacidad y Preocupaciones Éticas**: El NLP puede ser utilizado con fines no éticos (spam, bots, deepfakes).
:::

## Datos

::: fragment
Para este curso trabajaremos con datos de los diarios La Razón y Público de España

El archivo está en los links siguientes links: [acá](https://drive.google.com/drive/folders/1KQjyE2tKht7OWIZOJ1dyXKedtQdHjyxa?usp=sharing) o [acá](https://inechile.sharepoint.com/:f:/s/cienciadedatos/EtsXv7EPGoRHmv_hstSVCLUBr_ux4i2URQmoNvQh5raG7g?e=Im1tdd)
:::

::: {.fragment .medium-par}
Dos columnas relevantes:

1.  cuerpo
2.  titular
:::

::: fragment
```{r, include=T, eval=T, echo=T}
data <- read_csv("data/data_larazon_publico_v2.csv")
```
:::

::: fragment
<img src="imagenes/ejemplo_dataset.png" width="700"/>
:::

## Paquetes

::: {.fragment .medium-par}
En R existen varios paquetes para trabajar con datos de texto

-   tm
-   tidytext
-   stringr
-   spacyr (wrapper)
-   text2vec
:::

::: fragment
<img src="imagenes/logo_quanteda.png" width="400"/>

En este [sitio](https://tutorials.quanteda.io/) hay una documentación muy completa
:::

## Vectorizando el texto

::: {.incremental .medium-par}
-   El texto, tal cual, no se encuentra en un formato que sea entendible para un computador.

-   Como con los modelos de ML que practicamos, queremos encontrar una forma de que el texto termine siendo representado en una tabla.

-   Dos formas clásicas de lograrlo:
:::

::: fragment
**Vectorización en base a frecuencias y Word embeddings**
:::

## Vectorizando el texto: en base a frecuencias 

::: {.incremental .medium-par}
-   Las bolsas de palabras son formas sencillas de vectorizar texto

-   La más simple es la **bolsa de palabras**. Consiste simplemente en contar el número de veces que se repite cada palabra en el total de texto e ir incorporándolo en una tabla donde cada fila es un texto y cada columna una palabra

-   Veamos un ejemplo:
:::

::: fragment
```{r echo=T}

ejemplo <- data.frame(text = c("Mi gato es un tirano en casa. Él es amo y señor.",
                               "Soy esclavo de mi gato, pero lo amo.")) 
dfm_ejemplo <- ejemplo %>% 
  corpus() %>% 
  tokens() %>% 
  dfm()  

dfm_ejemplo 
```
:::

## Vectorizando el texto: en base a frecuencias



**Notas**

::: {.fragment .medium-par}
-   Pueden notar que el orden de las palabras no es relevante en este sistema -\> se pierde la secuencialidad del texto
:::

::: fragment
```{r echo=T}

ejemplo2 <- data.frame(text = c("Amo el hecho de que te odio", # Malvado
                               "Odio el hecho de que te amo"))# Romántico decepcionado
dfm_ejemplo2 <- ejemplo2 %>% 
  corpus() %>% # Un set de documentos es llamado un corpus
  tokens() %>% 
  dfm()  

dfm_ejemplo2
```
:::

## Vectorizando el texto: en base a frecuencias



**Notas**

::: {.incremental .medium-par}
-   Acá nuestro *token* de elección fue la palabra. Sin embargo, los tokens pueden ser otras combinaciones: carácteres, n-gramas, etc.

    -   Un n-grama es cada combinación de n palabras consecutivas en el texto. Repitiendo el mismo ejercicio con 2-gramas (o bigramas):
:::

::: fragment
```{r}
dfm_ejemplo <- ejemplo2 %>% 
  corpus() %>%  tokens() %>%
  tokens_ngrams(n = 2) %>% dfm

dfm_ejemplo 
```
:::

::: {.fragment .medium-par}
-   Se recupera un poco la secuencialidad a cambio de mayor **dimensionalidad**
:::

## Vectorizando el texto: en base a frecuencias



**Notas**

::: {.fragment .medium-par}
-   Mientras más palabras únicas existan en nuestro corpus, más columnas tendrá nuestra tabla y estará más poblada por ceros.
    -   Una tabla con un alto porcentaje de ceros es llamada matriz dispersa o *sparse matrix*
-   Juntemos los dos ejemplos anteriores y veamos cómo aumenta la dimensionalidad:
:::

::: r-stack
::: {.fragment .fade-in-then-out}
```{r echo=T}
dfm_ejemplo <- ejemplo %>% bind_rows(ejemplo2) %>% 
  corpus() %>%  tokens()  %>%
  dfm

dfm_ejemplo 
```
:::

::: {.fragment .fade-in-then-out}
```{r echo=T}

dfm_ejemplo <- ejemplo %>% bind_rows(ejemplo2) %>% 
  corpus() %>%  tokens() %>%
  tokens_ngrams(n = 2 ) %>%
  dfm %>% print(max_nfeat = 6)

```
:::
:::

## Vectorizando el texto: en base a frecuencias {auto-animate="true"}



**Notas**

::: {.incremental .medium-par}
-   Solo tenemos 4 frases cortas y nuestra matriz ya tiene 23-29 columnas.

-   Pueden imaginar lo rápido que esto puede descontrolarse con más ejemplos y textos más largos

-   Además, palabras comunes toman mucha relevancia en nuestra matriz, minimizando el impacto de palabras que entregan más información.

-   Una forma de vectorizar texto que se hace cargo parcialmente es TF-IDF:

::: fragment
**Term frequency - Inverse Document Frequency**
:::
:::

## Vectorizando el texto: en base a frecuencias {auto-animate="true"}



**Term frequency - Inverse Document Frequency**

::: fragment
TF = n° de veces que token t aparece en el documento d / Número total de tokens en documento d
:::

::: fragment
IDF = log(N° total de documentos / N° de documentos que contienen el token t)
:::

::: fragment
$$
TF-IDF(t,d) = TF(t,d)*IDF(t)
$$
:::

::: fragment
Lo importante es que esta medida le quita importancia a palabras que aparecen en todos los documentos. Así, los términos que aparecen en un documento específico, pero no en el resto del corpus, serán considerados más relevantes.
:::

::: fragment
Sin embargo, no se hace cargo del contexto en que se encuentran las palabras.
:::

## Vectorizando el texto: en base a frecuencias



**Term frequency - Inverse Document Frequency**

La implementación es sencilla: agregamos *dfm_tfidf* luego de *dfm*

::: fragment
```{r echo=T}

ejemplo <- data.frame(text = c("La casa es grande.",
                               "El árbol es chico.",
                               "La cama es blanda.",
                               "El pollo es sabroso.",
                               "La cama es grande.")) 

dfm_ejemplo <- ejemplo  %>% 
  corpus() %>%  tokens()  %>% dfm %>% 
  dfm_tfidf %>% print(max_nfeat = 8)

```
:::

## Preprocesamiento de texto



::: {.incremental .medium-par}
-   Mencionamos que en la medida que agregamos más documentos y términos, la dimensionalidad del problema comienza a crecer considerablemente.
-   Esto puede empezar a representar un problema en términos de costo computacional de realizar las operaciones que queramos realizar.
-   Podemos minimizar el problema preprocesando el texto.
-   Algunos pasos clásicos:
    -   Eliminar signos de puntuación
    -   Eliminar símbolos extraños
    -   Eliminar palabras poco significativas (stopwords)
    -   Eliminar palabras con menos de 3 caracteres
    -   Eliminar palabras que aparecen menos de 10 veces en el corpus
:::

## Preprocesamiento de texto



```{r echo=T}
set.seed(123)
tokens_data_complete <- data %>% 
  select(text = cuerpo) %>% 
  sample_frac(0.2) %>% 
  corpus() %>% 
  tokens()  
print(tokens_data_complete %>% dfm() %>% dim)

tokens_data_proc <- data %>% 
  select(text = cuerpo) %>% 
  sample_frac(0.2) %>% 
  corpus() %>% 
  tokens( remove_punct = TRUE, remove_symbols = TRUE) %>% 
  tokens_select( pattern = stopwords("es"), selection = "remove", min_nchar=3L)  

dfm_sample <- tokens_data_proc %>% 
  dfm() %>% 
  dfm_trim(min_termfreq = 10) 
print(dfm_sample %>% dim)
```


## Explorando datasets

```{r echo=T , fig.height=3 }
largo_textos <-  map_int(tokens_data_proc, length)
df_largo <- data.frame(largo = largo_textos)
df_largo %>% 
  ggplot(aes(x = largo)) +
  geom_histogram(binwidth = 15) +
  theme_bw()

print(summary(df_largo$largo))
```

## Explorando datasets

Palabras más y menos comunes

::: fragment
```{r echo=T}
dfm_sample %>% 
  quanteda::topfeatures(n = 10)
```
:::


::: fragment
```{r echo=T}
dfm_sample %>% 
  quanteda::topfeatures(n = 10, decreasing = F)
```
:::

## Explorando datasets
 
Búsqueda de palabras clave:

```{r echo=T}
comentarios_eta <-  tokens_data_complete %>% 
  kwic( pattern = "eta",  window = 7) # <<

head(comentarios_eta) %>% crear_tabla
```


## Vectorizando el texto: word embeddings

::: {.incremental .medium-par}
-   Los *word embeddings* son una forma de representación de las palabras, que busca mantener el sentido semántico de las palabras, mapeándolas como vectores en un espacio multi-dimensional.
-   A diferencia de la vectorización en base a frecuencias, estos vectores son densos y de largo fijo.
-   Consideran la relación con otras palabras del corpus, lo que permite agregar información de contexto y semántico.
-   La idea clave es la siguiente: palabras que se usan en contextos similares, tendrán representaciones vectoriales similares.
    -   Es decir, ocuparán un espacio similar dentro del espacio vectorial.
-   Además capturan la relación semántica de las palabras: "reina" y "rey" tendrán una distancia similar a "mujer" y "hombre".
    -   podemos aplicar ecuaciones como "rey - hombre + mujer", lo que nos entregaría la palabra "reina" si estos embeddings fueron entrenados con un corpus suficientemente grande.
:::

## Vectorizando el texto: word embeddings



:::: r-stack


![](imagenes/reina.png){.fragment .fade-out width="90%"}



![](imagenes/most_similar_perro.png){.fragment .fade-in-then-out width="90%"}



::::



## Vectorizando el texto: word embeddings



<img src="imagenes/embeddings.png" width="90%"/>

## Vectorizando el texto: word embeddings



::: columns
::: {.column .small-par .incremental style="flex: 0.55"}
-   No nos vamos a detener a ver cómo se entrenan, ya que existen *embeddings* pre-entrenados con cantidades de texto muy superiores a las que podríamos entrenar nosotros.

-   [En el siguiente link](https://github.com/dccuchile/spanish-word-embeddings) podemos encontrar *embeddings* de 300 dimensiones entrenados con texto en español.

-   Esto implica que cada palabra será representada por un vector que contiene 300 números distintos. Las dimensiones no tienen una interpretación, solo sabemos que vectores parecidos hacen alusión a palabras parecidas.

:::

::: {.column style="flex: 0.4"}
<img src="imagenes/dcc_embeddings.png" width="100%"/>
:::
:::

## Vectorizando el texto: word embeddings




::: {.incremental .medium-par}
- Implementaremos los _word embeddings_  en Python, ya que es donde
hay más desarrollo tanto de NLP como de _Deep Learning_

- No entraremos en más detalle, ya que lo más relevante es que podemos inyectar los
_embeddings_ dentro de una arquitectura de redes neuronales de forma de "entregarle" a nuestra red un conocimiento base avanzado del significado de las palabras, apoyando y mejorando el sistema.

- La implementación, afortunadamente, es sencilla.
:::

## Otros tópicos

::: {.incremental .medium-par}

- Lo visto es solo una pincelada. Otros temas que no vimos:

  - Análisis de sentimiento
  - Reconocimiento de entidades nombradas (NER)
  - Modelamiento de tópicos
  - Generación de lenguaje
  - Etc

:::

## La Tarea

::: {.incremental .medium-par}

- Recapitulando:
  - Tenemos un conocimiento introductorio de Machine Learning y NLP
  - Por lo tanto, ya podemos aplicar el conocimiento adquirido a datos de CAENES.
  - Similar a la vez anterior, les entregaré un set de entrenamiento de training y de
  test (esta vez con sus etiquetas).
  - El plan es aplicar lo aprendido en la sección de vectorización de texto en base a
  frecuencias para generar el mejor modelo posible.
  - Es relevante ver si sus modelos pueden superar el rendimiento actual de las 
  redes neuronales con el volumen de datos que tenemos actualmente
  - Algunos tips:
    - Datos de oficio y tareas podrían incorporarse concatenando las glosas
      - Fíjense que queden bien concatenadas (que la última palabra y la primera no
      queden juntadas con un "_")
    - Recuerden preprocesar el texto
    - Pueden probar distintos modelos, distintos enfoques (bolsa de palabras vs td-idf),
    distintos hiperparámetros.

:::



# 

<img src="imagenes/logo_portada2.png" width="20%"/>

[**Mentoría Censo: Codificación**]{.big-par .center-justified}

[**Proyecto Ciencia de Datos**]{.big-par .center-justified}

[**Introducción al Procesamiento de Lenguaje Natural**]{.big-par .center-justified}

[**octubre 2023**]{.big-par .center-justified}
